<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Xiaolong  Li</title>
<meta name="description" content="A simple, whitespace theme for academics. Xiaolong Li, virginia tech phd student, computer vision and 3D scene understanding
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/">

<!-- Theming-->




    
<!-- MathJax -->
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
        <!-- Social Icons -->
        <div class="row ml-1 ml-sm-0">
          <span class="contact-icon text-center">
  <a href="mailto:%6C%78%69%61%6F%6C%39@%76%74.%65%64%75"><i class="fas fa-envelope"></i></a>
  
  <a href="https://scholar.google.com/citations?user=xriXZ6kAAAAJ&hl" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>
  
  
  <a href="https://github.com/dragonlong" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
  <a href="https://www.linkedin.com/in/xiaolong-li19" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a>
  <a href="https://twitter.com/lxiaol9" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>
  
  
  
  
  
</span>

        </div>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              about
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item ">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     <span class="font-weight-bold">Xiaolong</span>   Li
    </h1>
     <p class="desc"><a href="#">lxiaol9@vt.edu</a>.</p>
  </header>

  <article>
    
    <div class="profile float-right">
      
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/me.jpg">
      
      
    </div>
    

    <div class="clearfix">
      <p>I am an Applied Scientist in Amazon, working on <strong>foundational 3D vision</strong> and <strong>video generation</strong>. I obtained Ph.D. student in computer engineering at <a href="https://vt.edu/">Virginia Tech</a>, advised by <a href="https://ece.vt.edu/people/profile/abbott">Prof. A. Lynn Abbott</a>, with research focused on deep 3D representations learning for dynamic scene understanding. I’m interested in AR/VR, Embodied AI, robotics.</p>

<p>I gained my bachelor degree in <a href="http://english.hust.edu.cn/">Huazhong University of Science and Technology</a>, leading the team on algorithms development and system
integration for drone control at the Mechanical Innovational Base(MIB) in <a href="http://qiming.hust.edu.cn">Qiming College</a>.</p>

<p>During the summer 2019, I am lucky to work with <a href="https://shurans.github.io/">Prof. Shuran Song</a>(now Stanford University), <a href="https://hughw19.github.io/">Dr. He Wang</a>(now Peking University), <a href="https://ericyi.github.io/">Dr. Li Yi</a> (Google Research, now Tsinghua University), and <a href="https://research.google/people/105767/">Johnny Chung Lee</a>(Google Brain Robotics) as a student ML researcher in Google Brain Robotics, Mountain View; in 2020 spring, I did a research internship on 3D perception in MERL, mentored by <a href="https://users.ece.cmu.edu/~sihengc/">Prof. Siheng Chen</a>(now Shanghai Jiaotong University), <a href="https://www.merl.com/people/sullivan">Dr. Alan Sullivan</a>(MERL); in 2021 summer, I worked with <a href="https://www.microsoft.com/en-us/research/people/ischakra/">Dr. Ishani Chakraborty</a>(Hololens), <a href="http://people.csail.mit.edu/yalesong/home/">Dr. Yale Song</a>(MSR), <a href="https://btekin.github.io/">Dr. Bugra Tekin</a>(Hololens) in a research internship. I have also worked with <a href="https://renayuki.wixsite.com/3doptics/people">Prof. Yunhui Zhu</a>(VT 3D Optics Group) on X-ray phase imaging.</p>

    </div>

    
      <div class="news">
  <h2>News</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
      
      
        <tr>
          <th scope="row">May 16, 2023</th>
          <td>
            
              Named as Outstanding Reviewer for CVPR 2023

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jun 27, 2022</th>
          <td>
            
              Joined AWS AI as an applied scientist working on 3D Vision!

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Sep 28, 2021</th>
          <td>
            
              My first submission to NeurIPS 2021 accepted, check paper <a href="https://arxiv.org/pdf/2111.00190.pdf">here</a>!

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">May 17, 2021</th>
          <td>
            
              Starting my research internship in Hololens, Microsoft

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Sep 21, 2020</th>
          <td>
            
              Our method ranked 3rd on <a href="http://semantic-kitti.org/index.html">SemanticKitti</a> Multi-sweep Semantic Segmentation Challenge!

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Mar 13, 2020</th>
          <td>
            
              <a href="https://articulated-pose.github.io/">One paper</a> accepted to CVPR 2020 as Oral presentation!

            
          </td>
        </tr>
      
      </table>
    </div>
  
</div>

    

    
      <!--INDUSTRY DESCRIPTION -->
 <div class="flex-container">
    <h2>Education</h2>
     <div class="row">
           <div class="col-sm-4 ml-auto">
               <p><img class="img-fluid" src="assets/img/VT.png" alt="" style="height:120px"></p>
               <p>PhD Student <br/> Aug. 2016-present</p>
           </div>
           <div class="col-sm-4">
               <p><img class="img-fluid" src="assets/img/hust.png" alt="" style="height:120px"></p>
               <p>Bachelor Degree <br/> Aug. 2012- June 2016</p>
           </div>
     </div><!--/.row -->
     <br/>
     <hr>
 </div><!--/.container -->

    

    
      <!--INDUSTRY DESCRIPTION -->
 <div class="flex-container">
    <h2>Industry</h2>
     <div class="row">
        <div class="col-sm-2.1 ml-auto">
            <p><img class="img-fluid" src="assets/img/aws_logo.png" alt="" style="height:120px"></p>
            <p class="text-center">Applied Scientist <br/> Summer 2022-Present</p>
        </div>
           <div class="col-sm-2.1 ml-auto">
               <p><img class="img-fluid" src="assets/img/microsoft.svg" alt="" style="height:120px"></p>
               <p class="text-center">Research Intern <br/> Summer 2021</p>
           </div>
           <div class="col-sm-2.1 ml-auto">
               <p><img class="img-fluid" src="assets/img/merl.png" alt="" style="height:120px"></p>
               <p class="text-center">Research Intern <br/> Spring 2020</p>
           </div>
           <div class="col-sm-2.1 ml-auto">
               <p><img class="img-fluid" src="assets/img/google.png" alt="" style="height:120px"></p>
               <p class="text-center">Student Researcher <br/> Summer 2019</p>
           </div>
           <div class="col-sm-2.1 ml-auto">
               <p><img class="img-fluid" src="assets/img/robotmeta.JPG" alt="" style="height:120px"></p>
               <p class="text-center">Research Intern <br/> Summer 2018</p>
           </div>
     </div><!--/.row -->
     <br/>
     <hr>
 </div><!--/.container -->

    
    
    
      <div class="publications">
  <h2>Publications</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-xs-1 col-sm-3">
  
  
    <img class="img-fluid z-depth-1 rounded" src="/assets/img/grounded_dreamer.png">
  
  </div>

  <div id="li2024grounded" class="col-xs-3 col-sm-9">
    
      <div class="author">
        
          
          
          
          
            
              
                
                
          

          
            
              
                <b><font color="darkblue">Li, Xiaolong</font></b>,
              
            
          
        
          
          
          
          

          
            
              
                
                  Mo, Jiawei,
                
              
            
          
        
          
          
          
          
            
              
            
          

          
            
              
                
                  Wang, Ying,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Parameshwara, Chethan,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Fei, Xiaohan,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Swaminathan, Ashwin,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Taylor, CJ,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Tu, Zhuowen,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Favaro, Paolo,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  and Soatto, Stefano
                
              
            
          
        
      <div class="title">Grounded Compositional and Diverse Text-to-3D with Pretrained Multi-View Diffusion Model</div>
      </div>

      <div class="periodical">
      
        <b><font color="darkblue">arXiv preprint arXiv:2404.18065</font></b>
      
      
      
        <b><font color="darkblue">2024</font></b>
      
      </div>
    

    <div>
    
    </div>

    <div class="links">
    
      <a href="" class="btn btn-sm z-depth-0" role="button">[project]</a>
    
    
      <a href="https://arxiv.org/pdf/2404.18065" class="btn btn-sm z-depth-0" role="button">[Paper]</a>
    
    
      <a href="" class="btn btn-sm z-depth-0" role="button">[Code]</a>
    
    
      <a class="abstract btn btn-sm z-depth-0" role="button">[Abstract]</a>
    
    
    

    
    
    
    
    

    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We propose an effective two-stage approach named Grounded-Dreamer to generate 3D assets
that can accurately follow complex, compositional text
prompts while achieving high fidelity by using a pre-trained
multi-view diffusion model..</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-xs-1 col-sm-3">
  
  
    <img class="img-fluid z-depth-1 rounded" src="/assets/img/hoitrack.png">
  
  </div>

  <div id="chen2023tracking" class="col-xs-3 col-sm-9">
    
      <div class="author">
        
          
          
          
          

          
            
              
                
                  Chen, Jiayi,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Yan, Mi,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Zhang, Jiazhao,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Xu, Yinzhen,
                
              
            
          
        
          
          
          
          
            
              
                
                
          

          
            
              
                <b><font color="darkblue">Li, Xiaolong</font></b>,
              
            
          
        
          
          
          
          
            
              
                
                
          

          
            
              
                
                  <a href="https://halfsummer11.github.io/">Weng, Yijia</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          

          
            
              
                
                  <a href="https://cs.stanford.edu/~ericyi/">Yi, Li</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          

          
            
              
                
                  <a href="https://www.cs.columbia.edu/~shurans/">Song, Shuran</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          

          
            
              
                
                  and <a href="https://hughw19.github.io/">Wang, He</a>
                
              
            
          
        
      <div class="title">Tracking and reconstructing hand object interactions from point cloud sequences in the wild</div>
      </div>

      <div class="periodical">
      
        <b><font color="darkblue">In Proceedings of the AAAI Conference on Artificial Intelligence</font></b>
      
      
      
        <b><font color="darkblue">2023</font></b>
      
      </div>
    

    <div>
    
    </div>

    <div class="links">
    
      <a href="https://pku-epic.github.io/HOtrack/" class="btn btn-sm z-depth-0" role="button">[project]</a>
    
    
      <a href="https://arxiv.org/abs/2209.12009" class="btn btn-sm z-depth-0" role="button">[Paper]</a>
    
    
      <a href="https://github.com/PKU-EPIC/HOTrack" class="btn btn-sm z-depth-0" role="button">[Code]</a>
    
    
      <a class="abstract btn btn-sm z-depth-0" role="button">[Abstract]</a>
    
    
    

    
    
    
    
    

    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We tackle the challenging task of jointly tracking hand object pose and reconstructing their shapes from depth point cloud sequences in the wild, given the initial poses at frame 0.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-xs-1 col-sm-3">
  
  
    <img class="img-fluid z-depth-1 rounded" src="/assets/img/foundation_3d.png">
  
  </div>

  <div id="parameshwara2023towards" class="col-xs-3 col-sm-9">
    
      <div class="author">
        
          
          
          
          

          
            
              
                
                  Parameshwara, Chethan,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Achille, Alessandro,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Trager, Matthew,
                
              
            
          
        
          
          
          
          
            
              
                
                
          

          
            
              
                <b><font color="darkblue">Li, Xiaolong</font></b>,
              
            
          
        
          
          
          
          

          
            
              
                
                  Mo, Jiawei,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Swaminathan, Ashwin,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Taylor, CJ,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Venkatraman, Dheera,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Fei, Xiaohan,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  and Soatto, Stefano
                
              
            
          
        
      <div class="title">Towards visual foundational models of physical scenes</div>
      </div>

      <div class="periodical">
      
        <b><font color="darkblue">arXiv preprint arXiv:2306.03727</font></b>
      
      
      
        <b><font color="darkblue">2023</font></b>
      
      </div>
    

    <div>
    
    </div>

    <div class="links">
    
      <a href="" class="btn btn-sm z-depth-0" role="button">[project]</a>
    
    
      <a href="https://arxiv.org/pdf/2306.03727" class="btn btn-sm z-depth-0" role="button">[Paper]</a>
    
    
      <a href="" class="btn btn-sm z-depth-0" role="button">[Code]</a>
    
    
      <a class="abstract btn btn-sm z-depth-0" role="button">[Abstract]</a>
    
    
    

    
    
    
    
    

    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We describe a first step towards learning general-purpose visual representations of physical scenes.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-xs-1 col-sm-3">
  
  
    <img class="img-fluid z-depth-1 rounded" src="/assets/img/napl_eccv.png">
  
  </div>

  <div id="zhao2022number" class="col-xs-3 col-sm-9">
    
      <div class="author">
        
          
          
          
          

          
            
              
                
                  Zhao, Yangheng,
                
              
            
          
        
          
          
          
          
            
              
            
          

          
            
              
                
                  Wang, Jun,
                
              
            
          
        
          
          
          
          
            
              
                
                
          

          
            
              
                <b><font color="darkblue">Li, Xiaolong</font></b>,
              
            
          
        
          
          
          
          

          
            
              
                
                  Hu, Yue,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Zhang, Ce,
                
              
            
          
        
          
          
          
          
            
              
            
          

          
            
              
                
                  Wang, Yanfeng,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  and Chen, Siheng
                
              
            
          
        
      <div class="title">Number-adaptive prototype learning for 3d point cloud semantic segmentation</div>
      </div>

      <div class="periodical">
      
        <b><font color="darkblue">In European Conference on Computer Vision</font></b>
      
      
      
        <b><font color="darkblue">2022</font></b>
      
      </div>
    

    <div>
    
    </div>

    <div class="links">
    
      <a href="" class="btn btn-sm z-depth-0" role="button">[project]</a>
    
    
      <a href="https://arxiv.org/abs/2210.09948" class="btn btn-sm z-depth-0" role="button">[Paper]</a>
    
    
      <a href="" class="btn btn-sm z-depth-0" role="button">[Code]</a>
    
    
      <a class="abstract btn btn-sm z-depth-0" role="button">[Abstract]</a>
    
    
    

    
    
    
    
    

    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Category-level object pose estimation aims to find 6D object.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-xs-1 col-sm-3">
  
  
    <img class="img-fluid z-depth-1 rounded" src="/assets/img/pointmotionnet.png">
  
  </div>

  <div id="wang2022pointmotionnet" class="col-xs-3 col-sm-9">
    
      <div class="author">
        
          
          
          
          
            
              
            
          

          
            
              
                
                  Wang, Jun,
                
              
            
          
        
          
          
          
          
            
              
                
                
          

          
            
              
                <b><font color="darkblue">Li, Xiaolong</font></b>,
              
            
          
        
          
          
          
          

          
            
              
                
                  Sullivan, Alan,
                
              
            
          
        
          
          
          
          
            
              
            
              
            
          

          
            
              
                
                  Abbott, Lynn,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  and Chen, Siheng
                
              
            
          
        
      <div class="title">Pointmotionnet: Point-wise motion learning for large-scale lidar point clouds sequences</div>
      </div>

      <div class="periodical">
      
        <b><font color="darkblue">In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</font></b>
      
      
      
        <b><font color="darkblue">2022</font></b>
      
      </div>
    

    <div>
    
    </div>

    <div class="links">
    
      <a href="" class="btn btn-sm z-depth-0" role="button">[project]</a>
    
    
      <a href="https://openaccess.thecvf.com/content/CVPR2022W/WAD/papers/Wang_PointMotionNet_Point-Wise_Motion_Learning_for_Large-Scale_LiDAR_Point_Clouds_Sequences_CVPRW_2022_paper.pdf" class="btn btn-sm z-depth-0" role="button">[Paper]</a>
    
    
      <a href="" class="btn btn-sm z-depth-0" role="button">[Code]</a>
    
    
      <a class="abstract btn btn-sm z-depth-0" role="button">[Abstract]</a>
    
    
    

    
    
    
    
    

    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Category-level object pose estimation aims to find 6D object poses of previously unseen object instances from known categories without access to object CAD models. To reduce the huge amount of pose annotations needed for category-level learning, we propose for the first time a self-supervised learning framework to estimate category-level 6D object pose from single 3D point clouds. During training, our method assumes no ground-truth pose annotations, no CAD models, and no multi-view supervision. The key to our method is to disentangle shape and pose through an invariant shape reconstruction module and an equivariant pose estimation module, empowered by SE(3) equivariant point cloud networks. The invariant shape reconstruction module learns to perform aligned reconstructions, yielding a category-level reference frame without using any annotations. In addition,the equivariant pose estimation module achieves category-level pose estimation accuracy that is comparable to some fully supervised methods. Extensive experiments demonstrate the effectiveness of our approach on both complete and partialdepth point clouds from the ModelNet40 benchmark, and on real depth point cloudsfrom the NOCS-REAL 275 dataset.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-xs-1 col-sm-3">
  
  
    <img class="img-fluid z-depth-1 rounded" src="/assets/img/neurips.png">
  
  </div>

  <div id="li2021leveraging" class="col-xs-3 col-sm-9">
    
      <div class="author">
        
          
          
          
          
            
              
                
                
          

          
            
              
                <b><font color="darkblue">Li, Xiaolong</font></b>,
              
            
          
        
          
          
          
          
            
              
                
                
          

          
            
              
                
                  <a href="https://halfsummer11.github.io/">Weng, Yijia</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          

          
            
              
                
                  <a href="https://cs.stanford.edu/~ericyi/">Yi, Li</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          

          
            
              
                
                  <a href="https://geometry.stanford.edu/member/guibas/index.html">Guibas, Leonidas</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          

          
            
              
                
                  <a href="https://ece.vt.edu/people/profile/abbott">Abbott, A Lynn</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          

          
            
              
                
                  <a href="https://www.cs.columbia.edu/~shurans/">Song, Shuran</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          

          
            
              
                
                  and <a href="https://hughw19.github.io/">Wang, He</a>
                
              
            
          
        
      <div class="title">Leveraging SE (3) Equivariance for Self-Supervised Category-Level Object Pose Estimation</div>
      </div>

      <div class="periodical">
      
        <b><font color="darkblue">NeurIPS</font></b>
      
      
      
        <b><font color="darkblue">2021</font></b>
      
      </div>
    

    <div>
    
    </div>

    <div class="links">
    
      <a href="https://dragonlong.github.io/equi-pose/" class="btn btn-sm z-depth-0" role="button">[project]</a>
    
    
      <a href="https://arxiv.org/pdf/2111.00190.pdf" class="btn btn-sm z-depth-0" role="button">[Paper]</a>
    
    
      <a href="https://github.com/dragonlong/equi-pose" class="btn btn-sm z-depth-0" role="button">[Code]</a>
    
    
      <a class="abstract btn btn-sm z-depth-0" role="button">[Abstract]</a>
    
    
    

    
    
    
    
    

    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Category-level object pose estimation aims to find 6D object poses of previously unseen object instances from known categories without access to object CAD models. To reduce the huge amount of pose annotations needed for category-level learning, we propose for the first time a self-supervised learning framework to estimate category-level 6D object pose from single 3D point clouds. During training, our method assumes no ground-truth pose annotations, no CAD models, and no multi-view supervision. The key to our method is to disentangle shape and pose through an invariant shape reconstruction module and an equivariant pose estimation module, empowered by SE(3) equivariant point cloud networks. The invariant shape reconstruction module learns to perform aligned reconstructions, yielding a category-level reference frame without using any annotations. In addition,the equivariant pose estimation module achieves category-level pose estimation accuracy that is comparable to some fully supervised methods. Extensive experiments demonstrate the effectiveness of our approach on both complete and partialdepth point clouds from the ModelNet40 benchmark, and on real depth point cloudsfrom the NOCS-REAL 275 dataset.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-xs-1 col-sm-3">
  
  
    <img class="img-fluid z-depth-1 rounded" src="/assets/img/cvpr20_2.png">
  
  </div>

  <div id="li2020category" class="col-xs-3 col-sm-9">
    
      <div class="author">
        
          
          
          
          
            
              
                
                
          

          
            
              
                <b><font color="darkblue">Li, Xiaolong</font></b>,
              
            
          
        
          
          
          
          
            
              
                
                
          

          
            
              
                
                  <a href="https://hughw19.github.io/">Wang, He</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          

          
            
              
                
                  <a href="https://cs.stanford.edu/~ericyi/">Yi, Li</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          

          
            
              
                
                  <a href="https://geometry.stanford.edu/member/guibas/index.html">Guibas, Leonidas J</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          

          
            
              
                
                  <a href="https://ece.vt.edu/people/profile/abbott">Abbott, A Lynn</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          

          
            
              
                
                  and <a href="https://www.cs.columbia.edu/~shurans/">Song, Shuran</a>
                
              
            
          
        
      <div class="title">Category-Level Articulated Object Pose Estimation</div>
      </div>

      <div class="periodical">
      
        <b><font color="darkblue">CVPR</font></b>
      
      
      
        <b><font color="darkblue">2020</font></b>
      
      </div>
    

    <div>
    
      <b><font color="firebrick">Oral Presentation(5.1%)</font></b>
    
    </div>

    <div class="links">
    
      <a href="https://articulated-pose.github.io/" class="btn btn-sm z-depth-0" role="button">[project]</a>
    
    
      <a href="https://arxiv.org/pdf/1912.11913.pdf" class="btn btn-sm z-depth-0" role="button">[Paper]</a>
    
    
      <a href="https://github.com/dragonlong/articulated-pose" class="btn btn-sm z-depth-0" role="button">[Code]</a>
    
    
      <a class="abstract btn btn-sm z-depth-0" role="button">[Abstract]</a>
    
    
    

    
    
    
    
    

    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper addresses the task of category-level pose
      estimation for articulated objects from a single depth image.
      We present a novel category-level approach that correctly
      accommodates object instances previously unseen during
      training. We introduce Articulation-aware Normalized
      Coordinate Space Hierarchy (ANCSH) – a canonical
      representation for different articulated objects in a given
      category. As the key to achieve intra-category general-
      ization, the representation constructs a canonical object
      space as well as a set of canonical part spaces. The
      canonical object space normalizes the object orientation,
      scales and articulations (e.g. joint parameters and states)
      while each canonical part space further normalizes its part
      pose and scale. We develop a deep network based on
      PointNet++ that predicts ANCSH from a single depth point
      cloud, including part segmentation, normalized coordi-
      nates, and joint parameters in the canonical object space.
      By leveraging the canonicalized joints, we demonstrate: 1)
      improved performance in part pose and scale estimations
      using the induced kinematic constraints from joints; 2) high
      accuracy for joint parameter estimation in camera space</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-xs-1 col-sm-3">
  
  
    <img class="img-fluid z-depth-1 rounded" src="/assets/img/isbi.png">
  
  </div>

  <div id="porwal2020idrid" class="col-xs-3 col-sm-9">
    
      <div class="author">
        
          
          
          
          

          
            
              
                
                  Porwal, Prasanna,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Pachade, Samiksha,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Kokare, Manesh,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Deshmukh, Girish ..,
                
              
            
          
        
          
          
          
          
            
              
                
                
          

          
            
              
                <b><font color="darkblue">Li, Xiaolong</font></b>,
              
            
          
        
          
          
          
          

          
            
              
                
                  and others, 
                
              
            
          
        
      <div class="title">Idrid: Diabetic retinopathy–segmentation and grading challenge</div>
      </div>

      <div class="periodical">
      
        <b><font color="darkblue">Medical image analysis</font></b>
      
      
      
        <b><font color="darkblue">2020</font></b>
      
      </div>
    

    <div>
    
    </div>

    <div class="links">
    
    
      <a href="https://www.sciencedirect.com/science/article/abs/pii/S1361841519301033" class="btn btn-sm z-depth-0" role="button">[Paper]</a>
    
    
      <a href="https://github.com/dragonlong/DR_blend" class="btn btn-sm z-depth-0" role="button">[Code]</a>
    
    
      <a class="abstract btn btn-sm z-depth-0" role="button">[Abstract]</a>
    
    
    

    
    
    
    
    

    
      <br>
      <br>
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Diabetic Retinopathy (DR) is the most common cause of avoidable vision loss, predominantly affecting the working-age population across the globe. Screening for DR, coupled with timely consultation and treatment, is a globally trusted policy to avoid vision loss. However, implementation of DR screening programs is challenging due to the scarcity of medical professionals able to screen a growing global diabetic population at risk for DR. Computer-aided disease diagnosis in retinal image analysis could provide a sustainable approach for such large-scale screening effort. The recent scientific advances in computing capacity and machine learning approaches provide an avenue for biomedical scientists to reach this goal. Aiming to advance the state-of-the-art in automatic DR diagnosis, a grand challenge on “Diabetic Retinopathy – Segmentation and Grading” was organized in conjunction with the IEEE International Symposium on Biomedical Imaging (ISBI - 2018). In this paper, we report the set-up and results of this challenge that is primarily based on Indian Diabetic Retinopathy Image Dataset (IDRiD). There were three principal sub-challenges: lesion segmentation, disease severity grading, and localization of retinal landmarks and segmentation. These multiple tasks in this challenge allow to test the generalizability of algorithms, and this is what makes it different from existing ones. It received a positive response from the scientific community with 148 submissions from 495 registrations effectively entered in this challenge. This paper outlines the challenge, its organization, the dataset used, evaluation methods and results of top-performing participating solutions. The top-performing approaches utilized a blend of clinical information, data augmentation, and an ensemble of models. These findings have the potential to enable new developments in retinal image analysis and image-based DR screening in particular.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-xs-1 col-sm-3">
  
  
    <img class="img-fluid z-depth-1 rounded" src="/assets/img/spie17.png">
  
  </div>

  <div id="wu2017texture" class="col-xs-3 col-sm-9">
    
      <div class="author">
        
          
          
          
          
            
              
                
                
          

          
            
              
                
                  <a href="http://optics.mit.edu/ziling-wu-phd">Wu, Ziling</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          

          
            
              
                <b><font color="darkblue">Li, Xiaolong</font></b>,
              
            
          
        
          
          
          
          
            
              
                
                
          

          
            
              
                
                  and <a href="https://renayuki.wixsite.com/3doptics">Zhu, Yunhui</a>
                
              
            
          
        
      <div class="title">Texture orientation-resolving imaging with structure illumination</div>
      </div>

      <div class="periodical">
      
        <b><font color="darkblue">In Computational Imaging II</font></b>
      
      
      
        <b><font color="darkblue">2017</font></b>
      
      </div>
    

    <div>
    
    </div>

    <div class="links">
    
    
      <a href="" class="btn btn-sm z-depth-0" role="button">[Paper]</a>
    
    
    
      <a class="abstract btn btn-sm z-depth-0" role="button">[Abstract]</a>
    
    
    

    
    
    
    
    

    
      <br>
      <br>
    
    
      <br>
      <br>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p></p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-xs-1 col-sm-3">
  
  
    <img class="img-fluid z-depth-1 rounded" src="/assets/img/cas15.png">
  
  </div>

  <div id="chen2015research" class="col-xs-3 col-sm-9">
    
      <div class="author">
        
          
          
          
          

          
            
              
                
                  Chen, Muhao,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Gong, Chen,
                
              
            
          
        
          
          
          
          
            
              
                
                
          

          
            
              
                <b><font color="darkblue">Li, Xiaolong</font></b>,
              
            
          
        
          
          
          
          

          
            
              
                
                  and Yu, Zongxin
                
              
            
          
        
      <div class="title">Research on solving Traveling Salesman Problem based on virtual instrument technology and genetic-annealing algorithms</div>
      </div>

      <div class="periodical">
      
        <b><font color="darkblue">In 2015 Chinese Automation Congress (CAC)</font></b>
      
      
      
        <b><font color="darkblue">2015</font></b>
      
      </div>
    

    <div>
    
    </div>

    <div class="links">
    
    
      <a href="" class="btn btn-sm z-depth-0" role="button">[Paper]</a>
    
    
    
      <a class="abstract btn btn-sm z-depth-0" role="button">[Abstract]</a>
    
    
    

    
    
    
    
    

    
      <br>
      <br>
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p></p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li></ol>
</div>

    

    
      <!--INDUSTRY DESCRIPTION -->
 <div class="news">
        <h2>SERVICES</h2>
         <div>
        <p>   I am a reviewer in  <b>JEI, TIP, ICCV 2021, ICLR 2022, CVPR 2022, CVPR 2023, ICML 2023, NeurIPS 2023, 3DV 2023, 3DV 2024. </b> </p>
        </div>
     <br/>
     <hr>
 </div><!--/.container -->

    

    
  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2024 Xiaolong  Li.
    Always love life, passionate, and kind.

    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html>
